{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1920d16",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2823d4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba84ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b68f6a",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbdcd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    raw = zip(kor, eng)\n",
    "    cleaned_corpus = set(raw)\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b732712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968 78968\n"
     ]
    }
   ],
   "source": [
    "kor_corpus, eng_corpus = zip(*cleaned_corpus)\n",
    "print(len(kor_corpus), len(eng_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6139a9d8",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba79e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, k_token = False, e_token = False):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    if k_token:\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!가-힣ㄱ-ㅎㅏ-ㅣ]+\", \" \", sentence)\n",
    "\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "    if e_token:\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "        sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa54c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korean data size: 78968\n",
      "english data size: 78968\n",
      "Korean: 아지즈 총리는 국가의 통치권은 보호될 것이며 어떤 외부의 간섭도 용납하지 않겠다 고 전했다 .\n",
      "English: The integrity and sovereignty of the country will be protected at all cost and no outside interference will be allowed , Aziz said .\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "for kor, eng in zip(kor_corpus, eng_corpus):\n",
    "    temp_kor = preprocess_sentence(kor, k_token = True)\n",
    "    temp_eng = preprocess_sentence(eng, e_token = True)\n",
    "\n",
    "    enc_corpus.append(temp_kor)\n",
    "    dec_corpus.append(temp_eng)\n",
    "    \n",
    "print('korean data size:', len(enc_corpus))\n",
    "print('english data size:', len(dec_corpus))\n",
    "print(\"Korean:\", enc_corpus[500])   \n",
    "print(\"English:\", dec_corpus[500]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6eb24",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d0398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentencepiece에 적용하기 위한 txt파일 생성\n",
    "\n",
    "with open('kor.txt', 'w') as f:\n",
    "    for sentence in kor_corpus:\n",
    "        f.write('{}\\n'.format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57170a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng.txt', 'w') as f:\n",
    "    for sentence in eng_corpus:\n",
    "        f.write('{}\\n'.format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdf5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(txt):\n",
    "    templates= '--input={} \\\n",
    "    --pad_id={} \\\n",
    "    --bos_id={} \\\n",
    "    --eos_id={} \\\n",
    "    --unk_id={} \\\n",
    "    --model_prefix={} \\\n",
    "    --vocab_size={} \\\n",
    "    --character_coverage={} \\\n",
    "    --model_type={}'\n",
    "    \n",
    "    train_input_file = txt\n",
    "    vocab_size = 20000 # vocab 사이즈\n",
    "    prefix = txt.split('.')[0] + '_spm' # 저장될 tokenizer 모델에 붙는 이름\n",
    "    pad_id=0 #<pad> token을 0으로 설정\n",
    "    bos_id=1 #<BOS> token을 1으로 설정\n",
    "    eos_id=2 #<EOS> token을 2으로 설정\n",
    "    unk_id=3 #<UNK> token(unknown)을 3으로 설정\n",
    "    character_coverage = 1.0 # to reduce character set\n",
    "    model_type ='unigram' # Choose from unigram (default), bpe, char, or word\n",
    "\n",
    "    cmd = templates.format(train_input_file,\n",
    "                           pad_id,\n",
    "                           bos_id,\n",
    "                           eos_id,\n",
    "                           unk_id,\n",
    "                           prefix,\n",
    "                           vocab_size,\n",
    "                           character_coverage,\n",
    "                           model_type)\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(prefix + '.model')\n",
    "    \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5beda056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=kor.txt     --pad_id=0     --bos_id=1     --eos_id=2     --unk_id=3     --model_prefix=kor_spm     --vocab_size=20000     --character_coverage=1.0     --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: kor.txt\n",
      "  input_format: \n",
      "  model_prefix: kor_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: kor.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5121800\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=2177\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78968 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 178419 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78968\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241435\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241435 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=94983 obj=14.8423 num_tokens=530428 num_tokens/piece=5.58445\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=84317 obj=13.4948 num_tokens=532916 num_tokens/piece=6.32039\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=63230 obj=13.5335 num_tokens=554409 num_tokens/piece=8.76813\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=63196 obj=13.491 num_tokens=554794 num_tokens/piece=8.77894\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=47396 obj=13.6784 num_tokens=583744 num_tokens/piece=12.3163\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=47396 obj=13.6349 num_tokens=583992 num_tokens/piece=12.3215\n",
      "unigram_model_tr"
     ]
    }
   ],
   "source": [
    "ko_tokenizer = generate_tokenizer('kor.txt')\n",
    "en_tokenizer = generate_tokenizer('eng.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6499b96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ainer.cc(505) LOG(INFO) EM sub_iter=0 size=35547 obj=13.8829 num_tokens=615670 num_tokens/piece=17.3199\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35547 obj=13.8313 num_tokens=615738 num_tokens/piece=17.3218\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26660 obj=14.1313 num_tokens=648768 num_tokens/piece=24.3349\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26660 obj=14.0744 num_tokens=648789 num_tokens/piece=24.3357\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=14.3013 num_tokens=671614 num_tokens/piece=30.5279\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=14.2607 num_tokens=671657 num_tokens/piece=30.5299\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: kor_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: kor_spm.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=eng.txt     --pad_id=0     --bos_id=1     --eos_id=2     --unk_id=3     --model_prefix=eng_spm     --vocab_size=20000     --character_coverage=1.0     --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: eng.txt\n",
      "  input_format: \n",
      "  model_prefix: eng_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: eng.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10653895\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=136\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78968 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 142775 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78968\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 109546\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 109546 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=55056 obj=11.393 num_tokens=232269 num_tokens/piece=4.21878\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46894 obj=9.07549 num_tokens=233630 num_tokens/piece=4.98209\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35168 obj=9.06319 num_tokens=246234 num_tokens/piece=7.00165\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35156 obj=9.04551 num_tokens=246451 num_tokens/piece=7.01021\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26367 obj=9.14369 num_tokens=266998 num_tokens/piece=10.1262\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26367 obj=9.12122 num_tokens=266988 num_tokens/piece=10.1258\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=9.20005 num_tokens=280732 num_tokens/piece=12.7605\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=9.1847 num_tokens=280724 num_tokens/piece=12.7602\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: eng_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: eng_spm.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.SetEncodeExtraOptions('bos:eos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a58e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = []\n",
    "tgt_train = []\n",
    "\n",
    "for i in range(0, len(kor_corpus)):\n",
    "    src_tokens = ko_tokenizer.EncodeAsIds(kor_corpus[i])\n",
    "    tgt_tokens = en_tokenizer.EncodeAsIds(eng_corpus[i])\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_train.append(src_tokens)\n",
    "    tgt_train.append(tgt_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ebe376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71985 71985\n",
      "(71985, 50)\n",
      "(71985, 50)\n"
     ]
    }
   ],
   "source": [
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_train, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_train, padding='post')\n",
    "\n",
    "print(len(enc_train), len(dec_train))\n",
    "\n",
    "print(enc_train.shape)\n",
    "print(dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d3c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6856 4672   18 7564   47  356 2472  593 5480 5350  794    4    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "[    1    17  1423  1516   161     4   812     9 11065     6    56    37\n",
      "  1289    13     7  2000   856     9  3554     5     2     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])\n",
    "print(dec_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5450ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./kor_spm.vocab', encoding='utf-8') as f:\n",
    "    Vo_k = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "kor_word2idx = {w[0]: i for i, w in enumerate(Vo_k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eddb1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./eng_spm.vocab', encoding='utf-8') as f:\n",
    "    Vo_e = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "eng_word2idx = {w[0]: i for i, w in enumerate(Vo_e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "678a2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(kor_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4218b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a744249",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': enc_train,\n",
    "        'dec_inputs': dec_train[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
    "    },\n",
    "    {\n",
    "        'outputs': dec_train[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80777904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ({inputs: (None, 50), dec_inputs: (None, 49)}, {outputs: (None, 49)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bcc4e",
   "metadata": {},
   "source": [
    "## Modeling - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b378b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6577960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ddf9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2efac445",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ecbc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51cf36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef4afa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb599fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66b6e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f842b1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88afdcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a7725ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b179ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e95a6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "540cebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126e16d",
   "metadata": {},
   "source": [
    "## Evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7effef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e9a70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence, k_token= True)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aadfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83e2d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['오바마는 대통령이다.', '시민들은 도시 속에 산다.', '커피는 필요 없다.', '일곱 명의 사망자가 발생했다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad6c65a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: top piracy resemble resemble transmission v chocolate plays tailor plays defendTuesdayTuesdayTuesday vribe plays acquir Tonight v Tonight plays Tonight v Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight plays minor grenadeTuesday Tonight Tonight Tonight Tonight Tonight Tonight Tonight\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: breach With With With With transmission Pixar transmission clouds interception defendTuesday RichmondTuesday RichmondTuesday Richmond Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight electric With Tonight Tonight Tonight Tonight Tonight Tonight Tonight Tonight\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: stitches stake Basilica dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite dynamite\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular Hudson Hudson microscop spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular spectacular\n"
     ]
    }
   ],
   "source": [
    "for sen in sentences:\n",
    "    translate(sen, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c19db06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_576/1649253655.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9801e9796c24014b6abe03e0e480437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama Obama is expected to be the president.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The man was the first time.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The couple is the first time.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: The death toll were killed in the attack.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c242e86060ac4dff8d33e7da5119a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama is expected to be a third term.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city of the city of the city.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee the coffee is a coffee.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: The deaths were killed in the deaths of the deaths.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e764c6805293444bb69d0372e4101b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama is a political challenge.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city's city of Rio city.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee, it doesn't want to get coffee.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: The seven deaths were killed in the attack.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b431c49b84aecae571f19b1908500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama was elected.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city is the highest mountain mountain.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The won't be surprised.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: The seven seven died of seven dead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d504aa0a21fe47adb0ac24c094a03620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama has been President Barack Obama.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city is a city of San Siro, the city's San city is a city of Santo city.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The prize is required.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: The seven-member toll was the rise of seven.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b273b2b7e9d410f9a27f406839bcf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: I expect President Barack Obama to be my office:\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city is a city township in towns with a mountain.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee bars\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: Seven seven deaths are dead, the report said.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b265b9c158ef476a8cf909ebe37079a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama can pick up.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain mountain.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The no Buy.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: Seven of the dead were destroyed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9efde7d4884ffd89bbc7a38990f94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: I President Barack Obama on Thursday.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: Many urban mountain do the city's mountain.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The won't get caught on drinking.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven deaths were among seven.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e97ab93544cd3910c14e025c8b3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: I President Bush has a way.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: The city mountain is a mountain guide.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The Praring of coffee neither need for\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven seven deaths were reported.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376f283ce6514cfb99e2158e0d7b0713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: Obama is expected to be the first of ever.\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: They have a mountain youths cut in the city.\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: The schedule never need for coffee.\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: Seven seven out of seven are expected.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    if epoch < EPOCHS:\n",
    "        for sen in sentences:\n",
    "            translate(sen, transformer, ko_tokenizer, en_tokenizer)\n",
    "    \n",
    "    else:\n",
    "        for sen in sentences:\n",
    "            translate(sen, transformer, ko_tokenizer, en_tokenizer, plot_attention = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a6a93",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4c8b12ad",
   "metadata": {},
   "source": [
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.  \n",
    " - **데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.**  \n",
    " \n",
    " \n",
    "2. Transformer 번역기 모델이 정상적으로 구동된다.  \n",
    " - **Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.**  \n",
    " \n",
    " \n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.  \n",
    " - **제시된 문장에 대한 그럴듯한 영어 번역문이 생성되었다.**\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d500d9",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4865cbd",
   "metadata": {},
   "source": [
    "1. https://github.com/miinkang/AI_Project_AIFFEL/blob/main/%5BGD-10%5Dtransformer_translation.ipynb\n",
    "2. https://github.com/hongdune/aiffel/blob/master/GD10_Transformer.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
